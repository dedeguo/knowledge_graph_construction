{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chende_ner_lab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dedeguo/knowledge_graph_construction/blob/master/chende_ner_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfe0dLcoxjOE",
        "colab_type": "code",
        "outputId": "3ddfe5e3-3eda-4345-fb4d-1560fe406bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WLlL78Kxo_Z",
        "colab_type": "code",
        "outputId": "0402fd2c-0b93-45c6-8df7-dadaf53dd02a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 17132146905020698054, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 4774318916497318080\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 12650617552032919929\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 11330115994\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 15469515894936499135\n",
              " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpxFJxRx0dTh",
        "colab_type": "code",
        "outputId": "4d76cc10-041b-4db1-ca93-6809aebc62ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jan 20 01:31:49 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.44       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-qNL0ds24yZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OIVLimbDfWE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==1.15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shB9vnidE1jX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install kashgari-tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn5abe9zK-Rw",
        "colab_type": "code",
        "outputId": "6248dda4-f7c9-4474-c0a6-cfebda725ece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "from kashgari.corpus import ChineseDailyNerCorpus\n",
        "\n",
        "train_x, train_y = ChineseDailyNerCorpus.load_data('train')\n",
        "valid_x, valid_y = ChineseDailyNerCorpus.load_data('valid')\n",
        "test_x, test_y = ChineseDailyNerCorpus.load_data('test')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:CUDA GPU available, you can set `kashgari.config.use_cudnn_cell = True` to use CuDNNCell. This will speed up the training, but will make model incompatible with CPU device.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://s3.bmio.net/kashgari/china-people-daily-ner-corpus.tar.gz\n",
            "2449408/2443473 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2SdxF_1LEfi",
        "colab_type": "code",
        "outputId": "e5c7e7d0-6890-4b1e-c10e-a9ccd35d1fbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import kashgari\n",
        "from kashgari.corpus import ChineseDailyNerCorpus\n",
        "from kashgari.tasks.labeling import BiLSTM_Model\n",
        "kashgari.config.use_cudnn_cell = True\n",
        "\n",
        "#\n",
        "train_x, train_y = ChineseDailyNerCorpus.load_data('train')\n",
        "test_x, test_y = ChineseDailyNerCorpus.load_data('test')\n",
        "valid_x, valid_y = ChineseDailyNerCorpus.load_data('valid')\n",
        "\n",
        "model = BiLSTM_Model()\n",
        "model.fit(train_x, train_y, valid_x, valid_y, epochs=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:CUDA GPU available, you can set `kashgari.config.use_cudnn_cell = True` to use CuDNNCell. This will speed up the training, but will make model incompatible with CPU device.\n",
            "WARNING:root:\n",
            "╭─────────────────────────────────────────────────────────────────────────╮\n",
            "│ ◎ ○ ○ ░░░░░░░░░░░░░░░░░░░░░  Important Message  ░░░░░░░░░░░░░░░░░░░░░░░░│\n",
            "├─────────────────────────────────────────────────────────────────────────┤\n",
            "│                                                                         │\n",
            "│              We renamed again for consistency and clarity.              │\n",
            "│                   From now on, it is all `kashgari`.                    │\n",
            "│  Changelog: https://github.com/BrikerMan/Kashgari/releases/tag/v1.0.0   │\n",
            "│                                                                         │\n",
            "│         | Backend          | pypi version   | desc           |          │\n",
            "│         | ---------------- | -------------- | -------------- |          │\n",
            "│         | TensorFlow 2.x   | kashgari 2.x.x | coming soon    |          │\n",
            "│         | TensorFlow 1.14+ | kashgari 1.x.x |                |          │\n",
            "│         | Keras            | kashgari 0.x.x | legacy version |          │\n",
            "│                                                                         │\n",
            "╰─────────────────────────────────────────────────────────────────────────╯\n",
            "\n",
            "WARNING:root:CuDNN enabled, this will speed up the training, but will make model incompatible with CPU device.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://s3.bmio.net/kashgari/china-people-daily-ner-corpus.tar.gz\n",
            "2449408/2443473 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Sequence length will auto set at 95% of sequence length\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 97)]              0         \n",
            "_________________________________________________________________\n",
            "layer_embedding (Embedding)  (None, 97, 100)           350000    \n",
            "_________________________________________________________________\n",
            "layer_blstm (Bidirectional)  (None, 97, 256)           235520    \n",
            "_________________________________________________________________\n",
            "layer_dropout (Dropout)      (None, 97, 256)           0         \n",
            "_________________________________________________________________\n",
            "layer_time_distributed (Time (None, 97, 8)             2056      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 97, 8)             0         \n",
            "=================================================================\n",
            "Total params: 587,576\n",
            "Trainable params: 587,576\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.2514 - acc: 0.9336Epoch 1/50\n",
            "327/327 [==============================] - 13s 40ms/step - loss: 0.2499 - acc: 0.9340 - val_loss: 0.1244 - val_acc: 0.9605\n",
            "Epoch 2/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0965 - acc: 0.9692Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0964 - acc: 0.9692 - val_loss: 0.0721 - val_acc: 0.9770\n",
            "Epoch 3/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9781Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0668 - acc: 0.9781 - val_loss: 0.0613 - val_acc: 0.9803\n",
            "Epoch 4/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9818Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0548 - acc: 0.9818 - val_loss: 0.0556 - val_acc: 0.9827\n",
            "Epoch 5/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9842Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0474 - acc: 0.9842 - val_loss: 0.0492 - val_acc: 0.9844\n",
            "Epoch 6/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9861Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0419 - acc: 0.9861 - val_loss: 0.0471 - val_acc: 0.9846\n",
            "Epoch 7/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9877Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0372 - acc: 0.9877 - val_loss: 0.0430 - val_acc: 0.9862\n",
            "Epoch 8/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9888Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0335 - acc: 0.9888 - val_loss: 0.0429 - val_acc: 0.9866\n",
            "Epoch 9/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9901Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0302 - acc: 0.9900 - val_loss: 0.0415 - val_acc: 0.9870\n",
            "Epoch 10/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9909Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0275 - acc: 0.9909 - val_loss: 0.0427 - val_acc: 0.9870\n",
            "Epoch 11/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9917Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0250 - acc: 0.9917 - val_loss: 0.0446 - val_acc: 0.9865\n",
            "Epoch 12/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9926Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0225 - acc: 0.9926 - val_loss: 0.0423 - val_acc: 0.9876\n",
            "Epoch 13/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0207 - acc: 0.9932Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0207 - acc: 0.9932 - val_loss: 0.0415 - val_acc: 0.9876\n",
            "Epoch 14/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0188 - acc: 0.9938Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0189 - acc: 0.9938 - val_loss: 0.0463 - val_acc: 0.9873\n",
            "Epoch 15/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0177 - acc: 0.9942Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0177 - acc: 0.9942 - val_loss: 0.0474 - val_acc: 0.9875\n",
            "Epoch 16/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9948Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0157 - acc: 0.9948 - val_loss: 0.0466 - val_acc: 0.9875\n",
            "Epoch 17/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9952Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0146 - acc: 0.9952 - val_loss: 0.0499 - val_acc: 0.9872\n",
            "Epoch 18/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0131 - acc: 0.9957Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0131 - acc: 0.9957 - val_loss: 0.0493 - val_acc: 0.9879\n",
            "Epoch 19/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9962Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0118 - acc: 0.9962 - val_loss: 0.0563 - val_acc: 0.9871\n",
            "Epoch 20/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0107 - acc: 0.9965Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0107 - acc: 0.9965 - val_loss: 0.0502 - val_acc: 0.9882\n",
            "Epoch 21/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0100 - acc: 0.9967Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0100 - acc: 0.9967 - val_loss: 0.0487 - val_acc: 0.9886\n",
            "Epoch 22/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0090 - acc: 0.9970Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0090 - acc: 0.9970 - val_loss: 0.0558 - val_acc: 0.9882\n",
            "Epoch 23/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9972Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0084 - acc: 0.9972 - val_loss: 0.0574 - val_acc: 0.9880\n",
            "Epoch 24/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9975Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0075 - acc: 0.9975 - val_loss: 0.0568 - val_acc: 0.9879\n",
            "Epoch 25/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9978Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0066 - acc: 0.9978 - val_loss: 0.0615 - val_acc: 0.9876\n",
            "Epoch 26/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9981Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0059 - acc: 0.9981 - val_loss: 0.0567 - val_acc: 0.9891\n",
            "Epoch 27/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9982Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.0675 - val_acc: 0.9877\n",
            "Epoch 28/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9984Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0050 - acc: 0.9984 - val_loss: 0.0651 - val_acc: 0.9879\n",
            "Epoch 29/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9985Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0687 - val_acc: 0.9879\n",
            "Epoch 30/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9986Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0042 - acc: 0.9986 - val_loss: 0.0618 - val_acc: 0.9877\n",
            "Epoch 31/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9987Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0672 - val_acc: 0.9882\n",
            "Epoch 32/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9988Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0664 - val_acc: 0.9882\n",
            "Epoch 33/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9989Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0702 - val_acc: 0.9879\n",
            "Epoch 34/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9990Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0739 - val_acc: 0.9881\n",
            "Epoch 35/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9989Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0748 - val_acc: 0.9882\n",
            "Epoch 36/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9991Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0747 - val_acc: 0.9882\n",
            "Epoch 37/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9993Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0023 - acc: 0.9993 - val_loss: 0.0765 - val_acc: 0.9883\n",
            "Epoch 38/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9993Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0752 - val_acc: 0.9883\n",
            "Epoch 39/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9993Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0736 - val_acc: 0.9882\n",
            "Epoch 40/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9992Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0799 - val_acc: 0.9880\n",
            "Epoch 41/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9993Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0775 - val_acc: 0.9881\n",
            "Epoch 42/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9993Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0801 - val_acc: 0.9881\n",
            "Epoch 43/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9992Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0787 - val_acc: 0.9881\n",
            "Epoch 44/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9995Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0791 - val_acc: 0.9887\n",
            "Epoch 45/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9988Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0761 - val_acc: 0.9886\n",
            "Epoch 46/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9995Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0015 - acc: 0.9995 - val_loss: 0.0817 - val_acc: 0.9881\n",
            "Epoch 47/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9997Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0796 - val_acc: 0.9886\n",
            "Epoch 48/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 9.1101e-04 - acc: 0.9997Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 9.1065e-04 - acc: 0.9997 - val_loss: 0.0779 - val_acc: 0.9888\n",
            "Epoch 49/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0010 - acc: 0.9997Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0010 - acc: 0.9997 - val_loss: 0.0792 - val_acc: 0.9889\n",
            "Epoch 50/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9996Epoch 1/50\n",
            "327/327 [==============================] - 6s 18ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0922 - val_acc: 0.9870\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa8063b7cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiOsWRP8Rk_c",
        "colab_type": "code",
        "outputId": "8af3fd92-0555-4d43-d490-cfce3d824142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import kashgari\n",
        "from kashgari.corpus import ChineseDailyNerCorpus\n",
        "from kashgari.tasks.labeling import BiLSTM_CRF_Model\n",
        "kashgari.config.use_cudnn_cell = True\n",
        "\n",
        "train_x, train_y = ChineseDailyNerCorpus.load_data('train')\n",
        "test_x, test_y = ChineseDailyNerCorpus.load_data('test')\n",
        "valid_x, valid_y = ChineseDailyNerCorpus.load_data('valid')\n",
        "\n",
        "model = BiLSTM_CRF_Model()\n",
        "model.fit(train_x, train_y, valid_x, valid_y, epochs=50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:CuDNN enabled, this will speed up the training, but will make model incompatible with CPU device.\n",
            "WARNING:root:Sequence length will auto set at 95% of sequence length\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 97)]              0         \n",
            "_________________________________________________________________\n",
            "layer_embedding (Embedding)  (None, 97, 100)           350000    \n",
            "_________________________________________________________________\n",
            "layer_blstm (Bidirectional)  (None, 97, 256)           235520    \n",
            "_________________________________________________________________\n",
            "layer_dense (Dense)          (None, 97, 64)            16448     \n",
            "_________________________________________________________________\n",
            "layer_crf_dense (Dense)      (None, 97, 8)             520       \n",
            "_________________________________________________________________\n",
            "layer_crf (CRF)              (None, 97, 8)             64        \n",
            "=================================================================\n",
            "Total params: 602,552\n",
            "Trainable params: 602,552\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 20.0295 - accuracy: 0.9344Epoch 1/50\n",
            "327/327 [==============================] - 84s 257ms/step - loss: 19.9949 - accuracy: 0.9344 - val_loss: 122.6065 - val_accuracy: 0.6210\n",
            "Epoch 2/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 6.5101 - accuracy: 0.9744Epoch 1/50\n",
            "327/327 [==============================] - 83s 253ms/step - loss: 6.5065 - accuracy: 0.9744 - val_loss: 118.9820 - val_accuracy: 0.4600\n",
            "Epoch 3/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 4.2072 - accuracy: 0.9817Epoch 1/50\n",
            "327/327 [==============================] - 83s 255ms/step - loss: 4.2040 - accuracy: 0.9818 - val_loss: 116.3990 - val_accuracy: 0.4593\n",
            "Epoch 4/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 3.2780 - accuracy: 0.9848Epoch 1/50\n",
            "327/327 [==============================] - 83s 253ms/step - loss: 3.2772 - accuracy: 0.9848 - val_loss: 114.6137 - val_accuracy: 0.4592\n",
            "Epoch 5/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 2.6568 - accuracy: 0.9872Epoch 1/50\n",
            "327/327 [==============================] - 83s 253ms/step - loss: 2.6604 - accuracy: 0.9871 - val_loss: 113.4643 - val_accuracy: 0.4518\n",
            "Epoch 6/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 2.1681 - accuracy: 0.9891Epoch 1/50\n",
            "327/327 [==============================] - 83s 254ms/step - loss: 2.1672 - accuracy: 0.9891 - val_loss: 113.6361 - val_accuracy: 0.4378\n",
            "Epoch 7/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 1.7907 - accuracy: 0.9907Epoch 1/50\n",
            "327/327 [==============================] - 84s 257ms/step - loss: 1.7907 - accuracy: 0.9907 - val_loss: 112.1843 - val_accuracy: 0.4502\n",
            "Epoch 8/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 1.5042 - accuracy: 0.9921Epoch 1/50\n",
            "327/327 [==============================] - 83s 254ms/step - loss: 1.5063 - accuracy: 0.9921 - val_loss: 111.0224 - val_accuracy: 0.4522\n",
            "Epoch 9/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 1.2708 - accuracy: 0.9930Epoch 1/50\n",
            "327/327 [==============================] - 83s 255ms/step - loss: 1.2699 - accuracy: 0.9930 - val_loss: 110.5174 - val_accuracy: 0.4545\n",
            "Epoch 10/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 1.0856 - accuracy: 0.9941Epoch 1/50\n",
            "327/327 [==============================] - 84s 257ms/step - loss: 1.0860 - accuracy: 0.9941 - val_loss: 110.8015 - val_accuracy: 0.4491\n",
            "Epoch 11/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.9106 - accuracy: 0.9950Epoch 1/50\n",
            "327/327 [==============================] - 83s 254ms/step - loss: 0.9102 - accuracy: 0.9950 - val_loss: 111.1031 - val_accuracy: 0.4483\n",
            "Epoch 12/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.7817 - accuracy: 0.9956Epoch 1/50\n",
            "327/327 [==============================] - 83s 253ms/step - loss: 0.7814 - accuracy: 0.9956 - val_loss: 111.1836 - val_accuracy: 0.4465\n",
            "Epoch 13/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.6660 - accuracy: 0.9963Epoch 1/50\n",
            "327/327 [==============================] - 83s 255ms/step - loss: 0.6649 - accuracy: 0.9963 - val_loss: 110.8479 - val_accuracy: 0.4470\n",
            "Epoch 14/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.5515 - accuracy: 0.9970Epoch 1/50\n",
            "327/327 [==============================] - 83s 255ms/step - loss: 0.5520 - accuracy: 0.9970 - val_loss: 111.7507 - val_accuracy: 0.4438\n",
            "Epoch 15/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.4642 - accuracy: 0.9975Epoch 1/50\n",
            "327/327 [==============================] - 83s 255ms/step - loss: 0.4639 - accuracy: 0.9975 - val_loss: 110.9747 - val_accuracy: 0.4485\n",
            "Epoch 16/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.4103 - accuracy: 0.9977Epoch 1/50\n",
            "327/327 [==============================] - 84s 255ms/step - loss: 0.4099 - accuracy: 0.9977 - val_loss: 112.0929 - val_accuracy: 0.4430\n",
            "Epoch 17/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.3359 - accuracy: 0.9983Epoch 1/50\n",
            "327/327 [==============================] - 84s 255ms/step - loss: 0.3358 - accuracy: 0.9983 - val_loss: 111.7548 - val_accuracy: 0.4459\n",
            "Epoch 18/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.2706 - accuracy: 0.9986Epoch 1/50\n",
            "327/327 [==============================] - 84s 258ms/step - loss: 0.2709 - accuracy: 0.9986 - val_loss: 112.3515 - val_accuracy: 0.4447\n",
            "Epoch 19/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.2303 - accuracy: 0.9988Epoch 1/50\n",
            "327/327 [==============================] - 84s 256ms/step - loss: 0.2298 - accuracy: 0.9988 - val_loss: 112.7808 - val_accuracy: 0.4376\n",
            "Epoch 20/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.1964 - accuracy: 0.9990Epoch 1/50\n",
            "327/327 [==============================] - 84s 257ms/step - loss: 0.1964 - accuracy: 0.9990 - val_loss: 113.0850 - val_accuracy: 0.4390\n",
            "Epoch 21/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.1828 - accuracy: 0.9989Epoch 1/50\n",
            "327/327 [==============================] - 83s 255ms/step - loss: 0.1827 - accuracy: 0.9989 - val_loss: 114.0684 - val_accuracy: 0.4364\n",
            "Epoch 22/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.1571 - accuracy: 0.9991Epoch 1/50\n",
            "327/327 [==============================] - 84s 257ms/step - loss: 0.1569 - accuracy: 0.9991 - val_loss: 113.3018 - val_accuracy: 0.4391\n",
            "Epoch 23/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.1422 - accuracy: 0.9992Epoch 1/50\n",
            "327/327 [==============================] - 84s 257ms/step - loss: 0.1423 - accuracy: 0.9992 - val_loss: 114.2215 - val_accuracy: 0.4327\n",
            "Epoch 24/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0947 - accuracy: 0.9995Epoch 1/50\n",
            "327/327 [==============================] - 84s 258ms/step - loss: 0.0947 - accuracy: 0.9995 - val_loss: 114.4810 - val_accuracy: 0.4370\n",
            "Epoch 25/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0940 - accuracy: 0.9995Epoch 1/50\n",
            "327/327 [==============================] - 85s 261ms/step - loss: 0.0939 - accuracy: 0.9995 - val_loss: 114.4740 - val_accuracy: 0.4352\n",
            "Epoch 26/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0716 - accuracy: 0.9996Epoch 1/50\n",
            "327/327 [==============================] - 85s 260ms/step - loss: 0.0716 - accuracy: 0.9996 - val_loss: 115.7474 - val_accuracy: 0.4295\n",
            "Epoch 27/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0641 - accuracy: 0.9997Epoch 1/50\n",
            "327/327 [==============================] - 85s 259ms/step - loss: 0.0640 - accuracy: 0.9997 - val_loss: 115.4783 - val_accuracy: 0.4353\n",
            "Epoch 28/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0917 - accuracy: 0.9994Epoch 1/50\n",
            "327/327 [==============================] - 85s 260ms/step - loss: 0.0917 - accuracy: 0.9994 - val_loss: 115.9304 - val_accuracy: 0.4312\n",
            "Epoch 29/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9992Epoch 1/50\n",
            "327/327 [==============================] - 86s 263ms/step - loss: 0.1125 - accuracy: 0.9992 - val_loss: 115.9723 - val_accuracy: 0.4336\n",
            "Epoch 30/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0704 - accuracy: 0.9995Epoch 1/50\n",
            "327/327 [==============================] - 85s 261ms/step - loss: 0.0703 - accuracy: 0.9995 - val_loss: 115.2485 - val_accuracy: 0.4365\n",
            "Epoch 31/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0715 - accuracy: 0.9996Epoch 1/50\n",
            "327/327 [==============================] - 86s 262ms/step - loss: 0.0713 - accuracy: 0.9996 - val_loss: 116.5250 - val_accuracy: 0.4303\n",
            "Epoch 32/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0266 - accuracy: 0.9999Epoch 1/50\n",
            "327/327 [==============================] - 86s 262ms/step - loss: 0.0265 - accuracy: 0.9999 - val_loss: 116.5105 - val_accuracy: 0.4309\n",
            "Epoch 33/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 1.0000Epoch 1/50\n",
            "327/327 [==============================] - 86s 262ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 115.8715 - val_accuracy: 0.4353\n",
            "Epoch 34/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0605 - accuracy: 0.9996Epoch 1/50\n",
            "327/327 [==============================] - 86s 262ms/step - loss: 0.0604 - accuracy: 0.9996 - val_loss: 116.9518 - val_accuracy: 0.4275\n",
            "Epoch 35/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0524 - accuracy: 0.9997Epoch 1/50\n",
            "327/327 [==============================] - 86s 263ms/step - loss: 0.0523 - accuracy: 0.9997 - val_loss: 116.7753 - val_accuracy: 0.4319\n",
            "Epoch 36/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0469 - accuracy: 0.9997Epoch 1/50\n",
            "327/327 [==============================] - 85s 261ms/step - loss: 0.0469 - accuracy: 0.9997 - val_loss: 118.3240 - val_accuracy: 0.4223\n",
            "Epoch 37/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0313 - accuracy: 0.9998Epoch 1/50\n",
            "327/327 [==============================] - 88s 269ms/step - loss: 0.0313 - accuracy: 0.9998 - val_loss: 116.8559 - val_accuracy: 0.4308\n",
            "Epoch 38/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0216 - accuracy: 0.9999Epoch 1/50\n",
            "327/327 [==============================] - 86s 264ms/step - loss: 0.0215 - accuracy: 0.9999 - val_loss: 118.0769 - val_accuracy: 0.4270\n",
            "Epoch 39/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9999Epoch 1/50\n",
            "327/327 [==============================] - 88s 269ms/step - loss: 0.0152 - accuracy: 0.9999 - val_loss: 117.9298 - val_accuracy: 0.4273\n",
            "Epoch 40/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0210 - accuracy: 0.9999Epoch 1/50\n",
            "327/327 [==============================] - 88s 269ms/step - loss: 0.0212 - accuracy: 0.9999 - val_loss: 118.8082 - val_accuracy: 0.4230\n",
            "Epoch 41/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0979 - accuracy: 0.9992Epoch 1/50\n",
            "327/327 [==============================] - 87s 265ms/step - loss: 0.0978 - accuracy: 0.9992 - val_loss: 119.1602 - val_accuracy: 0.4246\n",
            "Epoch 42/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0676 - accuracy: 0.9995Epoch 1/50\n",
            "327/327 [==============================] - 87s 266ms/step - loss: 0.0675 - accuracy: 0.9995 - val_loss: 117.9730 - val_accuracy: 0.4258\n",
            "Epoch 43/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0239 - accuracy: 0.9999Epoch 1/50\n",
            "327/327 [==============================] - 88s 268ms/step - loss: 0.0239 - accuracy: 0.9999 - val_loss: 118.0648 - val_accuracy: 0.4287\n",
            "Epoch 44/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0065 - accuracy: 1.0000Epoch 1/50\n",
            "327/327 [==============================] - 87s 267ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 118.8319 - val_accuracy: 0.4221\n",
            "Epoch 45/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0033 - accuracy: 1.0000Epoch 1/50\n",
            "327/327 [==============================] - 88s 268ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 118.7270 - val_accuracy: 0.4209\n",
            "Epoch 46/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0020 - accuracy: 1.0000Epoch 1/50\n",
            "327/327 [==============================] - 87s 266ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 117.6972 - val_accuracy: 0.4246\n",
            "Epoch 47/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0016 - accuracy: 1.0000Epoch 1/50\n",
            "327/327 [==============================] - 88s 268ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 117.8670 - val_accuracy: 0.4222\n",
            "Epoch 48/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0014 - accuracy: 1.0000Epoch 1/50\n",
            "327/327 [==============================] - 87s 266ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 117.7925 - val_accuracy: 0.4217\n",
            "Epoch 49/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000Epoch 1/50\n",
            "327/327 [==============================] - 87s 267ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 116.1358 - val_accuracy: 0.4289\n",
            "Epoch 50/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 9.6449e-04 - accuracy: 1.0000Epoch 1/50\n",
            "327/327 [==============================] - 88s 269ms/step - loss: 9.6180e-04 - accuracy: 1.0000 - val_loss: 117.6137 - val_accuracy: 0.4187\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa8053b9940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0SqkUFmhyPQ",
        "colab_type": "code",
        "outputId": "abaaf513-ae92-48da-9996-74ac215d6990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import kashgari\n",
        "from kashgari.corpus import ChineseDailyNerCorpus\n",
        "from kashgari.tasks.labeling import BiGRU_Model\t\n",
        "kashgari.config.use_cudnn_cell = True\n",
        "\n",
        "# 加载数据集\n",
        "train_x, train_y = ChineseDailyNerCorpus.load_data('train')\n",
        "test_x, test_y = ChineseDailyNerCorpus.load_data('test')\n",
        "valid_x, valid_y = ChineseDailyNerCorpus.load_data('valid')\n",
        "\n",
        "model = BiGRU_Model\t()\n",
        "model.fit(train_x, train_y, valid_x, valid_y, epochs=50)\n",
        "# 验证模型，此方法将打印出详细的验证报告\n",
        "model.evaluate(test_x, test_y)\n",
        "\n",
        "# 保存模型到 `saved_ner_model` 目录下\n",
        "model.save('saved_ner_model')\n",
        "\n",
        "# 加载保存模型\n",
        "loaded_model = kashgari.utils.load_model('saved_ner_model')\n",
        "\n",
        "# 使用模型进行预测\n",
        "loaded_model.predict(test_x[:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:CuDNN enabled, this will speed up the training, but will make model incompatible with CPU device.\n",
            "WARNING:root:Sequence length will auto set at 95% of sequence length\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, 97)]              0         \n",
            "_________________________________________________________________\n",
            "layer_embedding (Embedding)  (None, 97, 100)           350000    \n",
            "_________________________________________________________________\n",
            "layer_bgru (Bidirectional)   (None, 97, 256)           176640    \n",
            "_________________________________________________________________\n",
            "layer_dropout (Dropout)      (None, 97, 256)           0         \n",
            "_________________________________________________________________\n",
            "layer_time_distributed (Time (None, 97, 8)             2056      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 97, 8)             0         \n",
            "=================================================================\n",
            "Total params: 528,696\n",
            "Trainable params: 528,696\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9322Epoch 1/50\n",
            "327/327 [==============================] - 10s 31ms/step - loss: 0.2352 - acc: 0.9322 - val_loss: 0.1039 - val_acc: 0.9680\n",
            "Epoch 2/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9734Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0838 - acc: 0.9734 - val_loss: 0.0689 - val_acc: 0.9783\n",
            "Epoch 3/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9795Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0625 - acc: 0.9795 - val_loss: 0.0581 - val_acc: 0.9814\n",
            "Epoch 4/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9826Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0520 - acc: 0.9826 - val_loss: 0.0529 - val_acc: 0.9831\n",
            "Epoch 5/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9850Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0447 - acc: 0.9851 - val_loss: 0.0482 - val_acc: 0.9850\n",
            "Epoch 6/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9868Epoch 1/50\n",
            "327/327 [==============================] - 5s 17ms/step - loss: 0.0394 - acc: 0.9868 - val_loss: 0.0439 - val_acc: 0.9860\n",
            "Epoch 7/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9884Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0351 - acc: 0.9884 - val_loss: 0.0434 - val_acc: 0.9863\n",
            "Epoch 8/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9895Epoch 1/50\n",
            "327/327 [==============================] - 5s 17ms/step - loss: 0.0316 - acc: 0.9895 - val_loss: 0.0428 - val_acc: 0.9871\n",
            "Epoch 9/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9905Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0286 - acc: 0.9905 - val_loss: 0.0423 - val_acc: 0.9874\n",
            "Epoch 10/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0256 - acc: 0.9915Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0256 - acc: 0.9915 - val_loss: 0.0427 - val_acc: 0.9876\n",
            "Epoch 11/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0231 - acc: 0.9924Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0231 - acc: 0.9924 - val_loss: 0.0435 - val_acc: 0.9878\n",
            "Epoch 12/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9930Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0209 - acc: 0.9930 - val_loss: 0.0448 - val_acc: 0.9878\n",
            "Epoch 13/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0187 - acc: 0.9938Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0187 - acc: 0.9938 - val_loss: 0.0482 - val_acc: 0.9875\n",
            "Epoch 14/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0169 - acc: 0.9944Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0168 - acc: 0.9944 - val_loss: 0.0478 - val_acc: 0.9879\n",
            "Epoch 15/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0155 - acc: 0.9949Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0155 - acc: 0.9948 - val_loss: 0.0491 - val_acc: 0.9878\n",
            "Epoch 16/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9955Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0137 - acc: 0.9955 - val_loss: 0.0465 - val_acc: 0.9879\n",
            "Epoch 17/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0124 - acc: 0.9959Epoch 1/50\n",
            "327/327 [==============================] - 5s 16ms/step - loss: 0.0124 - acc: 0.9959 - val_loss: 0.0518 - val_acc: 0.9880\n",
            "Epoch 18/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9963Epoch 1/50\n",
            "327/327 [==============================] - 5s 17ms/step - loss: 0.0114 - acc: 0.9963 - val_loss: 0.0538 - val_acc: 0.9873\n",
            "Epoch 19/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9966Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0105 - acc: 0.9966 - val_loss: 0.0529 - val_acc: 0.9876\n",
            "Epoch 20/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9969Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0093 - acc: 0.9969 - val_loss: 0.0558 - val_acc: 0.9881\n",
            "Epoch 21/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0081 - acc: 0.9973Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0081 - acc: 0.9973 - val_loss: 0.0573 - val_acc: 0.9878\n",
            "Epoch 22/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9976Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0073 - acc: 0.9976 - val_loss: 0.0626 - val_acc: 0.9872\n",
            "Epoch 23/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9977Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0069 - acc: 0.9977 - val_loss: 0.0640 - val_acc: 0.9875\n",
            "Epoch 24/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0065 - acc: 0.9978Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0639 - val_acc: 0.9879\n",
            "Epoch 25/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9982Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.0630 - val_acc: 0.9884\n",
            "Epoch 26/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9984Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0678 - val_acc: 0.9872\n",
            "Epoch 27/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9984Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.0751 - val_acc: 0.9876\n",
            "Epoch 28/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9985Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.0710 - val_acc: 0.9878\n",
            "Epoch 29/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0039 - acc: 0.9987Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0723 - val_acc: 0.9879\n",
            "Epoch 30/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9989Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0702 - val_acc: 0.9884\n",
            "Epoch 31/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9990Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.0741 - val_acc: 0.9878\n",
            "Epoch 32/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9989Epoch 1/50\n",
            "327/327 [==============================] - 5s 17ms/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0748 - val_acc: 0.9880\n",
            "Epoch 33/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9989Epoch 1/50\n",
            "327/327 [==============================] - 5s 17ms/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0788 - val_acc: 0.9881\n",
            "Epoch 34/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9989Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0786 - val_acc: 0.9880\n",
            "Epoch 35/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9990Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0727 - val_acc: 0.9878\n",
            "Epoch 36/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9991Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.0765 - val_acc: 0.9879\n",
            "Epoch 37/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9993Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0785 - val_acc: 0.9881\n",
            "Epoch 38/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9994Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0816 - val_acc: 0.9881\n",
            "Epoch 39/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9994Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0830 - val_acc: 0.9882\n",
            "Epoch 40/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9994Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0018 - acc: 0.9994 - val_loss: 0.0811 - val_acc: 0.9884\n",
            "Epoch 41/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9994Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0017 - acc: 0.9994 - val_loss: 0.0901 - val_acc: 0.9880\n",
            "Epoch 42/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9994Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.0889 - val_acc: 0.9875\n",
            "Epoch 43/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9990Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0820 - val_acc: 0.9885\n",
            "Epoch 44/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9993Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0839 - val_acc: 0.9883\n",
            "Epoch 45/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9995Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0014 - acc: 0.9995 - val_loss: 0.0836 - val_acc: 0.9886\n",
            "Epoch 46/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9996Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0837 - val_acc: 0.9885\n",
            "Epoch 47/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9996Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0809 - val_acc: 0.9887\n",
            "Epoch 48/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9996Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0855 - val_acc: 0.9880\n",
            "Epoch 49/50\n",
            "324/327 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9994Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0859 - val_acc: 0.9886\n",
            "Epoch 50/50\n",
            "323/327 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9994Epoch 1/50\n",
            "327/327 [==============================] - 6s 17ms/step - loss: 0.0019 - acc: 0.9994 - val_loss: 0.0880 - val_acc: 0.9883\n",
            "           precision    recall  f1-score   support\n",
            "\n",
            "      LOC     0.7539    0.7809    0.7672      3428\n",
            "      ORG     0.6050    0.6873    0.6435      2146\n",
            "      PER     0.8060    0.8266    0.8162      1794\n",
            "\n",
            "micro avg     0.7197    0.7648    0.7415      7368\n",
            "macro avg     0.7232    0.7648    0.7431      7368\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Sequence length will auto set at 95% of sequence length\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['O',\n",
              "  'O',\n",
              "  'B-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['B-PER',\n",
              "  'I-PER',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORG',\n",
              "  'I-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-PER',\n",
              "  'I-PER',\n",
              "  'I-PER',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-LOC',\n",
              "  'I-LOC',\n",
              "  'O',\n",
              "  'B-LOC',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'I-LOC',\n",
              "  'I-LOC',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['B-PER',\n",
              "  'I-PER',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['O',\n",
              "  'B-PER',\n",
              "  'I-PER',\n",
              "  'I-PER',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhCp09KpoQ_Z",
        "colab_type": "code",
        "outputId": "44f475f7-656e-421c-aa9c-3b1e17e0a89c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "!ls\n",
        "!wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
        "!unzip chinese_L-12_H-768_A-12.zip\n",
        "!ls chinese_L-12_H-768_A-12"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  saved_ner_model\n",
            "--2019-12-18 11:04:28--  https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.197.128, 2607:f8b0:400e:c07::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.197.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 381892918 (364M) [application/zip]\n",
            "Saving to: ‘chinese_L-12_H-768_A-12.zip’\n",
            "\n",
            "chinese_L-12_H-768_ 100%[===================>] 364.20M  63.2MB/s    in 5.8s    \n",
            "\n",
            "2019-12-18 11:04:34 (63.2 MB/s) - ‘chinese_L-12_H-768_A-12.zip’ saved [381892918/381892918]\n",
            "\n",
            "Archive:  chinese_L-12_H-768_A-12.zip\n",
            "   creating: chinese_L-12_H-768_A-12/\n",
            "  inflating: chinese_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: chinese_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: chinese_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: chinese_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: chinese_L-12_H-768_A-12/bert_config.json  \n",
            "bert_config.json\t\t     bert_model.ckpt.index  vocab.txt\n",
            "bert_model.ckpt.data-00000-of-00001  bert_model.ckpt.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEnF4wPgrsih",
        "colab_type": "code",
        "outputId": "7ab8c1e4-9d12-4b8d-8b3b-75b37b90021c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from kashgari.corpus import ChineseDailyNerCorpus\n",
        "\n",
        "# 加载数据集\n",
        "train_x, train_y = ChineseDailyNerCorpus.load_data('train')\n",
        "test_x, test_y = ChineseDailyNerCorpus.load_data('test')\n",
        "valid_x, valid_y = ChineseDailyNerCorpus.load_data('valid')\n",
        "\n",
        "print(f\"train data count: {len(train_x)}\")\n",
        "print(f\"validate data count: {len(valid_x)}\")\n",
        "print(f\"test data count: {len(test_x)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data count: 20864\n",
            "validate data count: 2318\n",
            "test data count: 4636\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBJsSvAspsjS",
        "colab_type": "code",
        "outputId": "c232053f-6d1f-41e7-dffc-63650535e74d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import kashgari\n",
        "from kashgari.corpus import ChineseDailyNerCorpus\n",
        "from kashgari.tasks.labeling import BiGRU_Model\t\n",
        "from kashgari.embeddings import BERTEmbedding\n",
        "\n",
        "kashgari.config.use_cudnn_cell = True\n",
        "\n",
        "BERT_PATH = 'chinese_L-12_H-768_A-12'\n",
        "\n",
        "embedding = BERTEmbedding(BERT_PATH,\n",
        "                          task=kashgari.LABELING,\n",
        "                          sequence_length=100 )\n",
        "\n",
        "\n",
        "# 加载数据集，\n",
        "train_x, train_y = ChineseDailyNerCorpus.load_data('train')\n",
        "test_x, test_y = ChineseDailyNerCorpus.load_data('test')\n",
        "valid_x, valid_y = ChineseDailyNerCorpus.load_data('valid')\n",
        "\n",
        "model = BiGRU_Model\t(embedding)\n",
        "model.fit(train_x, train_y, valid_x, valid_y, epochs=50)\n",
        "# 验证模型\n",
        "model.evaluate(test_x, test_y)\n",
        "\n",
        "# 保存模型到目录下\n",
        "model.save('saved_ner_model_bert_BiGRU')\n",
        "\n",
        "# 加载保存模型\n",
        "loaded_model = kashgari.utils.load_model('saved_ner_model_bert_BiGRU')\n",
        "\n",
        "# 使用模型进行预测\n",
        "loaded_model.predict(test_x[:10])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:CuDNN enabled, this will speed up the training, but will make model incompatible with CPU device.\n",
            "WARNING:root:seq_len: 100\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 100)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 100, 768), ( 16226304    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 100, 768)     1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 100, 768)     0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 100, 768)     76800       Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 100, 768)     0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 100, 768)     1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 100, 768)     2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 100, 768)     0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 100, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 100, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 100, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 100, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 100, 768)     0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 100, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 100, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 100, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 100, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 100, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 100, 768)     0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 100, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 100, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 100, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 100, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 100, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 100, 768)     0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 100, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 100, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 100, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 100, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 100, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 100, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 100, 768)     0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 100, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 100, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-Output (Concatenate)    (None, 100, 3072)    0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "non_masking_layer (NonMaskingLa (None, 100, 3072)    0           Encoder-Output[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "layer_bgru (Bidirectional)      (None, 100, 256)     2459136     non_masking_layer[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_dropout (Dropout)         (None, 100, 256)     0           layer_bgru[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "layer_time_distributed (TimeDis (None, 100, 8)       2056        layer_dropout[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 100, 8)       0           layer_time_distributed[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 103,821,832\n",
            "Trainable params: 2,461,192\n",
            "Non-trainable params: 101,360,640\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9861Epoch 1/50\n",
            "327/327 [==============================] - 101s 308ms/step - loss: 0.0463 - acc: 0.9861 - val_loss: 0.0135 - val_acc: 0.9955\n",
            "Epoch 2/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0157 - acc: 0.9949Epoch 1/50\n",
            "327/327 [==============================] - 93s 285ms/step - loss: 0.0157 - acc: 0.9949 - val_loss: 0.0109 - val_acc: 0.9964\n",
            "Epoch 3/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0119 - acc: 0.9961Epoch 1/50\n",
            "327/327 [==============================] - 93s 285ms/step - loss: 0.0119 - acc: 0.9961 - val_loss: 0.0099 - val_acc: 0.9966\n",
            "Epoch 4/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9969Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0093 - acc: 0.9969 - val_loss: 0.0093 - val_acc: 0.9969\n",
            "Epoch 5/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9974Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0075 - acc: 0.9974 - val_loss: 0.0106 - val_acc: 0.9965\n",
            "Epoch 6/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9978Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0064 - acc: 0.9978 - val_loss: 0.0105 - val_acc: 0.9967\n",
            "Epoch 7/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.9981Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0055 - acc: 0.9981 - val_loss: 0.0095 - val_acc: 0.9971\n",
            "Epoch 8/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9984Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0047 - acc: 0.9984 - val_loss: 0.0105 - val_acc: 0.9966\n",
            "Epoch 9/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9985Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0043 - acc: 0.9985 - val_loss: 0.0097 - val_acc: 0.9970\n",
            "Epoch 10/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9985Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0043 - acc: 0.9985 - val_loss: 0.0114 - val_acc: 0.9969\n",
            "Epoch 11/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9987Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0110 - val_acc: 0.9968\n",
            "Epoch 12/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9987Epoch 1/50\n",
            "327/327 [==============================] - 93s 285ms/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0115 - val_acc: 0.9968\n",
            "Epoch 13/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9987Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0036 - acc: 0.9987 - val_loss: 0.0116 - val_acc: 0.9969\n",
            "Epoch 14/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0038 - acc: 0.9987Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0113 - val_acc: 0.9971\n",
            "Epoch 15/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9987Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0036 - acc: 0.9987 - val_loss: 0.0145 - val_acc: 0.9964\n",
            "Epoch 16/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9988Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0034 - acc: 0.9988 - val_loss: 0.0128 - val_acc: 0.9968\n",
            "Epoch 17/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9989Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0105 - val_acc: 0.9973\n",
            "Epoch 18/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0037 - acc: 0.9987Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.0114 - val_acc: 0.9970\n",
            "Epoch 19/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9989Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0105 - val_acc: 0.9972\n",
            "Epoch 20/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9989Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0119 - val_acc: 0.9968\n",
            "Epoch 21/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9991Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0127 - val_acc: 0.9972\n",
            "Epoch 22/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9990Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0131 - val_acc: 0.9971\n",
            "Epoch 23/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9988Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0035 - acc: 0.9988 - val_loss: 0.0160 - val_acc: 0.9967\n",
            "Epoch 24/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9989Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0145 - val_acc: 0.9969\n",
            "Epoch 25/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9990Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0126 - val_acc: 0.9971\n",
            "Epoch 26/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9990Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0123 - val_acc: 0.9970\n",
            "Epoch 27/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9991Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0155 - val_acc: 0.9969\n",
            "Epoch 28/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9991Epoch 1/50\n",
            "327/327 [==============================] - 93s 285ms/step - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0125 - val_acc: 0.9971\n",
            "Epoch 29/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9991Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0125 - val_acc: 0.9972\n",
            "Epoch 30/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.9990Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0027 - acc: 0.9990 - val_loss: 0.0141 - val_acc: 0.9968\n",
            "Epoch 31/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9990Epoch 1/50\n",
            "327/327 [==============================] - 93s 285ms/step - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0127 - val_acc: 0.9971\n",
            "Epoch 32/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9990Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0132 - val_acc: 0.9971\n",
            "Epoch 33/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9991Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0116 - val_acc: 0.9972\n",
            "Epoch 34/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9990Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0144 - val_acc: 0.9969\n",
            "Epoch 35/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9991Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0122 - val_acc: 0.9971\n",
            "Epoch 36/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9991Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0144 - val_acc: 0.9969\n",
            "Epoch 37/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9991Epoch 1/50\n",
            "327/327 [==============================] - 93s 285ms/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0139 - val_acc: 0.9969\n",
            "Epoch 38/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9991Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0126 - val_acc: 0.9972\n",
            "Epoch 39/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9991Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0124 - val_acc: 0.9972\n",
            "Epoch 40/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9992Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0160 - val_acc: 0.9968\n",
            "Epoch 41/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9992Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0130 - val_acc: 0.9971\n",
            "Epoch 42/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9991Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0131 - val_acc: 0.9972\n",
            "Epoch 43/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9990Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0028 - acc: 0.9990 - val_loss: 0.0129 - val_acc: 0.9970\n",
            "Epoch 44/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9991Epoch 1/50\n",
            "327/327 [==============================] - 93s 285ms/step - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0128 - val_acc: 0.9969\n",
            "Epoch 45/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9992Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0138 - val_acc: 0.9970\n",
            "Epoch 46/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9992Epoch 1/50\n",
            "327/327 [==============================] - 93s 285ms/step - loss: 0.0023 - acc: 0.9992 - val_loss: 0.0121 - val_acc: 0.9971\n",
            "Epoch 47/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9992Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0024 - acc: 0.9992 - val_loss: 0.0151 - val_acc: 0.9969\n",
            "Epoch 48/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9993Epoch 1/50\n",
            "327/327 [==============================] - 93s 284ms/step - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0138 - val_acc: 0.9970\n",
            "Epoch 49/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.9992Epoch 1/50\n",
            "327/327 [==============================] - 93s 283ms/step - loss: 0.0022 - acc: 0.9992 - val_loss: 0.0154 - val_acc: 0.9969\n",
            "Epoch 50/50\n",
            "326/327 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9993Epoch 1/50\n",
            "327/327 [==============================] - 92s 282ms/step - loss: 0.0021 - acc: 0.9993 - val_loss: 0.0140 - val_acc: 0.9970\n",
            "           precision    recall  f1-score   support\n",
            "\n",
            "      ORG     0.8540    0.8747    0.8642      2147\n",
            "      LOC     0.9242    0.9350    0.9296      3431\n",
            "      PER     0.9623    0.9661    0.9642      1797\n",
            "\n",
            "micro avg     0.9126    0.9250    0.9188      7375\n",
            "macro avg     0.9131    0.9250    0.9190      7375\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Sequence length will auto set at 95% of sequence length\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-LOC',\n",
              "  'I-LOC',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-LOC',\n",
              "  'I-LOC',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-LOC',\n",
              "  'B-LOC',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'B-LOC',\n",
              "  'I-LOC',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['B-PER',\n",
              "  'I-PER',\n",
              "  'I-PER',\n",
              "  'O',\n",
              "  'B-PER',\n",
              "  'I-PER',\n",
              "  'I-PER',\n",
              "  'O',\n",
              "  'B-PER',\n",
              "  'I-PER',\n",
              "  'I-PER',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-LOC',\n",
              "  'I-LOC',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-PER',\n",
              "  'I-PER',\n",
              "  'I-PER',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-LOC',\n",
              "  'I-LOC',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['B-LOC',\n",
              "  'I-LOC',\n",
              "  'B-LOC',\n",
              "  'I-LOC',\n",
              "  'B-PER',\n",
              "  'I-PER',\n",
              "  'I-PER',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-LOC',\n",
              "  'I-LOC',\n",
              "  'B-LOC',\n",
              "  'I-LOC',\n",
              "  'I-LOC',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-LOC',\n",
              "  'I-LOC',\n",
              "  'I-LOC',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-LOC',\n",
              "  'I-LOC',\n",
              "  'I-LOC',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}